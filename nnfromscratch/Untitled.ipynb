{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import time\n",
    "import os \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_ds = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (150, 4) y.shape: (150,)\n"
     ]
    }
   ],
   "source": [
    "X,y = iris_ds.data,iris_ds.target\n",
    "print(\"X.shape:\",X.shape,\"y.shape:\",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=.2,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.8, 4. , 1.2, 0.2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,Xtest,ytrain,ytest = Xtrain.T,Xtest.T,ytrain.T,ytest.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.8, 4. , 1.2, 0.2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain.shape: (4, 120) ytrain.shape: (120,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Xtrain.shape:\",Xtrain.shape,\"ytrain.shape:\",ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_target = {}\n",
    "for idx,target in enumerate(iris_ds.target_names):\n",
    "    idx_to_target[idx] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'setosa', 1: 'versicolor', 2: 'virginica'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs:  [[0]\n",
      " [1]\n",
      " [2]] \n",
      "Shape: (3, 1)\n"
     ]
    }
   ],
   "source": [
    "outputs = np.array(list(idx_to_target.keys()))\n",
    "outputs = outputs.reshape(-1,1)\n",
    "print(\"Outputs: \",outputs,\"\\nShape:\",outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample count: 120\n"
     ]
    }
   ],
   "source": [
    "m  = Xtrain.shape[1]\n",
    "print(\"Total sample count:\",m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>Observation:</b> There are 120 train samples which have 4 features. And also there are 120 label for train. Label is one of them \"setosa,versicolor,virginica\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self,layers=[4,5,3],learning_rate=0.001,iterations=100):\n",
    "        '''\n",
    "        layers[0] = input layer size\n",
    "        layers[1] = hidden layer size\n",
    "        layers[2] = output layer size\n",
    "        \n",
    "        '''\n",
    "        self.params = {}\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations # epoch \n",
    "        self.loss = []\n",
    "        self.sample_size = None\n",
    "        self.layers = layers\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        \n",
    "        \n",
    "    def init_weights(self):\n",
    "        self.w1 = np.random.rand(self.layers[1],self.layers[0])\n",
    "        self.b1 = np.random.rand(self.layers[1],1)\n",
    "        self.w2 = np.random.rand(self.layers[2],self.layers[1])\n",
    "        self.b2 = np.random.rand(self.layers[2],1)\n",
    "        self.params = {\"w1\":self.w1,\"b1\":self.b1,\"w2\":self.w2,\"b2\":self.b2}\n",
    "    \n",
    "    def relu(self,Z):\n",
    "        if Z > 0 :\n",
    "            return Z\n",
    "        else:\n",
    "            return 0 \n",
    "    \n",
    "    def softmax(self,Z):\n",
    "        score = np.exp(Z) / np.sum(np.exp(Z))  \n",
    "        return score \n",
    "    \n",
    "    def cross_entropy(self,y,yhat):\n",
    "        ce = np.multiply(np.log(yhat),y) + np.multiply(np.log(1-yhat),1-y)\n",
    "        return -ce \n",
    "    \n",
    "    def calc_cost(losses):\n",
    "        return np.mean(losses)\n",
    "    \n",
    "    def forward_propagation(self):\n",
    "        W1 = self.params[\"w1\"]\n",
    "        b1 = self.params[\"b1\"]\n",
    "        W2 = self.params[\"w2\"]\n",
    "        b2 = self.params[\"b2\"]\n",
    "        \n",
    "        z1 = np.dot(W1,self.X)+ b1\n",
    "        a1 = relu(z1)\n",
    "        z2 = np.dot(W2,a1) + b2\n",
    "        yhat = softmax(z2)\n",
    "        loss = cross_entropy(self.y,yhat)\n",
    "        \n",
    "        self.cache = {\"z1\":z1,\"a1\":a1,\"z2\":z2,\"yhat\":yhat}\n",
    "        return yhat,loss\n",
    "    \n",
    "    def back_propagation(self,yhat):\n",
    "        '''\n",
    "        compute derivative and update weights&biases\n",
    "        '''\n",
    "        W1 = self.params[\"w1\"]\n",
    "        b1 = self.params[\"b1\"]\n",
    "        W2 = self.params[\"w2\"]\n",
    "        b2 = self.params[\"b2\"]\n",
    "        \n",
    "        A1 = self.cache[\"a1\"]\n",
    "        # backward\n",
    "        dz2 = yhat - self.y \n",
    "        dw2 = np.dot(dz2,A1.T) / m\n",
    "        db2 = np.sum(dz2,axis=1,keepdims=True) / m\n",
    "        dz1 = np.multiply(np.dot(W2.T,dz2), (1-np.power(A1,2)))\n",
    "        dw1 = np.dot(dz1,self.X.T) / m \n",
    "        db1 = np.sum(dz1,axis=1,keepdims=True) / m\n",
    "    \n",
    "        w1 = W1 - self.learning_rate * dw1\n",
    "        b1 = b1 - self.learning_rate * db1\n",
    "        w2 = W2 - self.learning_rate * dw2\n",
    "        b2 = b2 - self.learning_rate * db2\n",
    "        \n",
    "        self.params = {\"w1\":self.w1,\"b1\":self.b1,\"w2\":self.w2,\"b2\":self.b2}\n",
    "        \n",
    "    def fit(X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.init_weights()\n",
    "        for i in range(self.iterations):\n",
    "            \n",
    "            yhat,loss = self.forward_propagation()\n",
    "            self.back_propagation()\n",
    "            \n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        Makes prediction using model on test data\n",
    "        '''\n",
    "        yhat,_ = self.forward_propagation()\n",
    "        return yhat\n",
    "    def acc(self,y,yhat):\n",
    "        acc = int( sum(y==yhat) / len(y) * 100)\n",
    "        return acc \n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.loss)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylable(\"logloss\")\n",
    "        plt.title(\"Loss cure for training\")\n",
    "        plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': array([[0.30317429, 0.22848073, 0.295245  , 0.8896095 ],\n",
       "        [0.72214467, 0.34268371, 0.77044781, 0.75890291],\n",
       "        [0.93913669, 0.04365552, 0.85200454, 0.82171998],\n",
       "        [0.82655218, 0.48307576, 0.85120366, 0.93437701],\n",
       "        [0.23762908, 0.16177229, 0.25060671, 0.34585422]]),\n",
       " 'b1': array([[0.62548028],\n",
       "        [0.99768911],\n",
       "        [0.74033982],\n",
       "        [0.9751418 ],\n",
       "        [0.36089635]]),\n",
       " 'w2': array([[0.98515662, 0.0640912 , 0.80152839, 0.67591329, 0.92865665],\n",
       "        [0.19616872, 0.34556914, 0.62539626, 0.95119408, 0.6343442 ],\n",
       "        [0.10485432, 0.81757139, 0.40204173, 0.52056518, 0.61021176]]),\n",
       " 'b2': array([[0.2729081 ],\n",
       "        [0.15787407],\n",
       "        [0.0341948 ]])}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-d77494d95a09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0myhat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-77-ef76e865a75c>\u001b[0m in \u001b[0;36mforward_propagation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mz1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mz2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "yhat,loss= nn.forward_propagation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Idea "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Idea:\n",
    "* Define Layers\n",
    "* Inialize layers weights\n",
    "* For Loop (Epoch/iteration)\n",
    "   * Forwad prop\n",
    "   * Compute loss\n",
    "   * Backward \n",
    "   * Update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,inputs,outputs,hidden_units=10,activation_func='tanh',output_activation_func='sigmoid'): \n",
    "        input_init = inputs.shape[1]\n",
    "        output_init = outputs.shape[0]\n",
    "        self.w1 = np.random.rand(input_init,hidden_units)\n",
    "        self.b1 = np.zeros((hidden_units,1))\n",
    "        self.w2 = np.random.rand(hidden_units,output_init)\n",
    "        self.b2 = np.zeros((output_init,1))\n",
    "        self.params = {\"w1\":self.w1,\"b1\":self.b1,\"w2\":self.w2,\"b2\":self.b2}\n",
    "        self.activation_function = activation_func\n",
    "        self.output_activation_function = output_activation_func\n",
    "    \n",
    "    def lineer_func(w,x,b):\n",
    "        '''\n",
    "        Args: x = input\n",
    "              w = weight\n",
    "              b = bias\n",
    "        '''\n",
    "        return np.dot(w,x)+b\n",
    "\n",
    "    def sigmoid(z):\n",
    "        '''\n",
    "        z = lineer result of a neuron \n",
    "        '''\n",
    "        sigmoid = 1 / (1 + np.exp(-z))\n",
    "        return sigmoid\n",
    "\n",
    "    def relu(z):\n",
    "        '''\n",
    "        z = lineer result of a neuron\n",
    "        '''\n",
    "        return max(0,z)\n",
    "\n",
    "    def non_lineer_func(lineer_result,func_type='sigmoid'):\n",
    "        '''\n",
    "        Args: lineer_result = it's Z; Result of lineer part of a neuron\n",
    "              func_type = help to decide which non-lineer func use to calculate output of neuron\n",
    "        '''\n",
    "        if func_type == 'sigmoid':\n",
    "            return sigmoid(lineer_result)\n",
    "        elif func_type == 'relu':\n",
    "            return relu(lineer_result)\n",
    "        elif func_type =='tanh':\n",
    "            return np.tanh(lineer_result)\n",
    "   \n",
    "    def forward_prop(self,X,params,verbose=False):\n",
    "        w1 = self.params[\"w1\"]\n",
    "        b1 = self.params[\"b1\"]\n",
    "        w2 = self.params[\"w2\"]\n",
    "        b2 = self.params[\"b2\"]\n",
    "\n",
    "        z1 = lineer_func(w1,X,b1) # lineer func\n",
    "        a1 = non_lineer_func(z1,self.activation_function ) # non lineer func\n",
    "        z2 = lineer_func(w2,a1,b2)\n",
    "        a2 = non_lineer_func(z2,self.output_activation_function)\n",
    "\n",
    "        self.cache = {\"z1\":z1,\"a1\":a1,\"z2\":z2,\"a2\":a2}\n",
    "        return a2,self.cache\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defineLayers(x,y):\n",
    "    '''\n",
    "    Function is find how many units in layers. So that i can define layer sizes.\n",
    "    Args:\n",
    "        X: inputs \n",
    "        y: targets ... for our task = [0,1,2]\n",
    "        \n",
    "    Returns: \n",
    "        n_x : input units count \n",
    "        n_h : hidden units counts\n",
    "        n_y : output units count\n",
    "    '''\n",
    "    n_x = x.shape[0] # input count \n",
    "    n_h = 5 # I defined it. So that my nn [inputs-> 3 hiddenunits -> outputs]\n",
    "    n_y = y.shape[0] # it's label count\n",
    "    return (n_x,n_h,n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_x = defineLayers(Xtrain,ytrain)[0]\n",
    "n_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "(n_x,n_h,n_y) = defineLayers(Xtrain,outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of input layer:  4\n",
      "Size of hidden layer:  5\n",
      "Size of output layer:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of input layer: \",n_x)\n",
    "print(\"Size of hidden layer: \",n_h)\n",
    "print(\"Size of output layer: \",n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initialize Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeLayers(n_x,n_h,n_y,verbose=False):\n",
    "    '''\n",
    "    Args:\n",
    "        n_x : size of inputs layer\n",
    "        n_h : size of hidden layer\n",
    "        n_y : size of output layer\n",
    "    \n",
    "    Returns : params which is include :\n",
    "              w1 and b1 are weight and bias values of 1st layer\n",
    "              w2 and b1 are weight and bias values of 2nd layer\n",
    "    '''\n",
    "    \n",
    "    w1 = np.random.rand(n_h,n_x) # check what will be when transpose  \n",
    "    b1 = np.zeros((n_h,1))\n",
    "    w2 = np.random.rand(n_y,n_h)\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    params = {\"w1\":w1,\"b1\":b1,\"w2\":w2,\"b2\":b2}\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Parameters:\\n\")\n",
    "        pprint(params)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = initializeLayers(n_x,n_h,n_y,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]),\n",
      " 'b2': array([[0.],\n",
      "       [0.],\n",
      "       [0.]]),\n",
      " 'w1': array([[0.92394921, 0.44808843, 0.35851763, 0.2561268 ],\n",
      "       [0.68898875, 0.3764789 , 0.49954545, 0.28850572],\n",
      "       [0.8653749 , 0.06730282, 0.72552444, 0.82108784],\n",
      "       [0.43731079, 0.81368443, 0.32922002, 0.32729991],\n",
      "       [0.282708  , 0.92883171, 0.81874574, 0.26451474]]),\n",
      " 'w2': array([[0.6780047 , 0.13443903, 0.75105624, 0.51425849, 0.74907619],\n",
      "       [0.45027238, 0.04491036, 0.1920904 , 0.99169396, 0.13082726],\n",
      "       [0.4216712 , 0.60789197, 0.52126299, 0.15094137, 0.24463749]])}\n"
     ]
    }
   ],
   "source": [
    "pprint(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. For Loop(Iteration/Epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineer_func(w,x,b):\n",
    "    '''\n",
    "    Args: x = input\n",
    "          w = weight\n",
    "          b = bias\n",
    "    '''\n",
    "    return np.dot(w,x)+b\n",
    "\n",
    "def sigmoid(z):\n",
    "    '''\n",
    "    z = lineer result of a neuron \n",
    "    '''\n",
    "    sigmoid = 1 / (1 + np.exp(-z))\n",
    "    return sigmoid\n",
    "\n",
    "def relu(z):\n",
    "    '''\n",
    "    z = lineer result of a neuron\n",
    "    '''\n",
    "    return max(0,z)\n",
    "\n",
    "def non_lineer_func(lineer_result,func_type='sigmoid'):\n",
    "    '''\n",
    "    Args: lineer_result = it's Z; Result of lineer part of a neuron\n",
    "          func_type = help to decide which non-lineer func use to calculate output of neuron\n",
    "    '''\n",
    "    if func_type == 'sigmoid':\n",
    "        return sigmoid(lineer_result)\n",
    "    elif func_type == 'relu':\n",
    "        return relu(lineer_result)\n",
    "    elif func_type =='tanh':\n",
    "        return np.tanh(lineer_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X,params,verbose=False):\n",
    "    '''\n",
    "        Args: X: inputs\n",
    "              params: weight and bias values\n",
    "        Return last_preds and cache(all calculations)\n",
    "                a2: last prediction\n",
    "                cache: a dict which contains all calculations\n",
    "    '''\n",
    "    w1 = params[\"w1\"]\n",
    "    b1 = params[\"b1\"]\n",
    "    w2 = params[\"w2\"]\n",
    "    b2 = params[\"b2\"]\n",
    "    \n",
    "    z1 = lineer_func(w1,X,b1) # lineer func\n",
    "    a1 = non_lineer_func(z1,'tanh') # non lineer func\n",
    "    z2 = lineer_func(w2,a1,b2)\n",
    "    a2 = non_lineer_func(z2,'sigmoid')\n",
    "\n",
    "    cache = {\"z1\":z1,\"a1\":a1,\"z2\":z2,\"a2\":a2}\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"X1:\",X)\n",
    "        print(\"X1:\",X.shape,\"\\tW1:\",w1.shape)\n",
    "        print(\"Z1:\",z1.shape,\"\\ta1:\",a1.shape)\n",
    "        print(\"X2:\",a1.shape,\"\\tW2:\",w2.shape)\n",
    "        print(\"Z2:\",z2.shape,\"\\ta2:\",a2.shape)\n",
    "        print(\"\\n*** Last Prediction ***\\n\")\n",
    "        pprint(cache[\"a2\"])\n",
    "        print(\"\\n*** CACHE ***\\n \")\n",
    "        pprint(cache)\n",
    "    return a2,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds,cache = forward_prop(Xtrain,params,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Compute Loss & Cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateLoss(y,yhat,verbose=False):\n",
    "    '''\n",
    "    Args : y = real value\n",
    "           yhat = prediction\n",
    "    '''\n",
    "    loss = np.multiply(np.log(yhat),y) + np.multiply(np.log(1-yhat),1-y)\n",
    "    if verbose:\n",
    "        print(\"Loss:\",-loss)\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCost(losses,verbose=False):\n",
    "    '''\n",
    "    losses = whole loss values\n",
    "    '''\n",
    "    cost = np.squeeze(np.mean(losses))\n",
    "    if verbose:\n",
    "        print(\"Cost:\",cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = calculateLoss(ytrain,preds,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = calculateCost(losses,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"losses:\\n\")\n",
    "pprint(losses)\n",
    "print(\"\\ncost:\",cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(x,y,params,cache,verbose=False):\n",
    "    m = y.shape[0]\n",
    "    w1 = params[\"w1\"]\n",
    "    w2 = params[\"w2\"]\n",
    "    a1 = cache[\"a1\"]\n",
    "    a2 = cache[\"a2\"]\n",
    "    \n",
    "    # backward\n",
    "    dz2 = a2 - y \n",
    "    #print(\"dz2:\",dz2)\n",
    "    \n",
    "    dw2 = np.dot(dz2,a1.T) / m\n",
    "    #print(\"dw2:\",dw2)\n",
    "    \n",
    "    db2 = np.sum(dz2,axis=1,keepdims=True) / m\n",
    "    #print(\"db2:\",db2)\n",
    "    \n",
    "    dz1 = np.multiply(np.dot(w2.T,dz2), (1-np.power(a1,2)))\n",
    "    #print(\"dz1:\",dz1)\n",
    "    \n",
    "    dw1 = np.dot(dz1,X.T) / m \n",
    "    #print(\"dw1:\",dw1)\n",
    "    \n",
    "    db1 = np.sum(dz1,axis=1,keepdims=True) / m\n",
    "    #print(\"db1:\",db1)\n",
    "    if verbose:\n",
    "        print(\"dz2:\",dz2)\n",
    "        print(\"dw2:\",dw2)\n",
    "        print(\"db2:\",db2)\n",
    "        print(\"dz1:\",dz1)\n",
    "        print(\"dw1:\",dw1)\n",
    "        print(\"db1:\",db1)\n",
    "    grads = {\"dw1\":dw1,\"db1\":db1,\"dw2\":dw2,\"db2\":db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5,120) and (4,150) not aligned: 120 (dim 1) != 4 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-a549464825ec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-419e61347edb>\u001b[0m in \u001b[0;36mbackward_prop\u001b[1;34m(x, y, params, cache, verbose)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m#print(\"dz1:\",dz1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mdw1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdz1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;31m#print(\"dw1:\",dw1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (5,120) and (4,150) not aligned: 120 (dim 1) != 4 (dim 0)"
     ]
    }
   ],
   "source": [
    "grads = backward_prop(Xtrain,ytrain,params,cache,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Gradients\")\n",
    "pprint(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Update Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, lr = 1.2,verbose=False):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    w1 = params[\"w1\"]\n",
    "    b1 = params[\"b1\"]\n",
    "    w2 = params[\"w2\"]\n",
    "    b2 = params[\"b2\"]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    dw1 = grads[\"dw1\"]\n",
    "    db1 = grads[\"dw1\"]\n",
    "    dw2 = grads[\"dw2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    w1 = w1 - lr * dw1\n",
    "    b1 = b1 - lr * db1\n",
    "    w2 = w2 - lr * dw2\n",
    "    b2 = b2 - lr * db2\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    params = {\"w1\": w1,\n",
    "                  \"b1\": b1,\n",
    "                  \"w2\": w2,\n",
    "                  \"b2\": b2}\n",
    "    if verbose:\n",
    "        print(\"Updated Parameters:\")\n",
    "        pprint(params)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parameters\")\n",
    "pprint(params)\n",
    "updated_params = update_parameters(params,grads,lr=2,verbose=False)\n",
    "print(\"\\n Updated Parameters\\n\")\n",
    "pprint(updated_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Neural Network Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain[0].reshape(-1,1).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNetwork(X, Y, n_h, num_iterations = 4, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    sample_x = X[0]\n",
    "    sample_y = y[0]\n",
    "    n_x,_,n_y = defineLayers(sample_x, sample_y)\n",
    "    params = initializeLayers(n_x, n_h, n_y)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_prop(X, params)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = calculateCost(A2, Y, params)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_prop(params, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        params = update_parameters(params, grads)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        print (\"Epoch {i+1} \\t Cost: {}\" %(i, cost))\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = NeuralNetwork(Xtrain,ytrain,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    pred,_ = forward_prop(X, params)\n",
    "    prediction = np.argmax(pred) # Find the index of max score \n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyLabel(pred,label_dict):\n",
    "    '''\n",
    "    Args : pred is prediction index generated by predict function\n",
    "           label_dict is a dict which contains index-label pairs \n",
    "    '''\n",
    "    return label_dict[pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

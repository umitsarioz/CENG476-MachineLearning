{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_ds = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (150, 4) y.shape: (150,)\n"
     ]
    }
   ],
   "source": [
    "X,y = iris_ds.data,iris_ds.target\n",
    "print(\"X.shape:\",X.shape,\"y.shape:\",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=.2,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain.shape: (120, 4) ytrain.shape: (120,)\n",
      "\n",
      "Xtrain[:5]:\n",
      "[[5.8 4.  1.2 0.2]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [4.4 2.9 1.4 0.2]]\n",
      "\n",
      "ytrain[:5]:\n",
      "[0 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Xtrain.shape:\",Xtrain.shape,\"ytrain.shape:\",ytrain.shape)\n",
    "print(f\"\\nXtrain[:5]:\\n{Xtrain[:5]}\\n\\nytrain[:5]:\\n{ytrain[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,Xtest,ytrain,ytest = Xtrain.T,Xtest.T,ytrain.reshape(-1,1).T,ytest.reshape(-1,1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain.shape: (4, 120) ytrain.shape: (1, 120)\n",
      "\n",
      "Xtrain[:5]:\n",
      "[[5.8 4.8 6.9 6.6 4.4]\n",
      " [4.  3.4 3.1 2.9 2.9]\n",
      " [1.2 1.9 5.4 4.6 1.4]\n",
      " [0.2 0.2 2.1 1.3 0.2]]\n",
      "\n",
      "ytrain[:5]:\n",
      "[[0 0 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Xtrain.shape:\",Xtrain.shape,\"ytrain.shape:\",ytrain.shape)\n",
    "print(f\"\\nXtrain[:5]:\\n{Xtrain[:,:5]}\\n\\nytrain[:5]:\\n{ytrain[:,:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n"
     ]
    }
   ],
   "source": [
    "idx_to_target = {}\n",
    "for idx,target in enumerate(iris_ds.target_names):\n",
    "    idx_to_target[idx] = target\n",
    "print(idx_to_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Layer Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_X: 4\n",
      "N_h: 5\n",
      "N_y: 3\n"
     ]
    }
   ],
   "source": [
    "n_h = 5  # hidden_layer_units\n",
    "n_x = len(Xtrain[:,0]) # input_layer_units \n",
    "n_y = len(idx_to_target) # output_layer_units \n",
    "\n",
    "print(\"N_X:\",n_x)\n",
    "print(\"N_h:\",n_h)\n",
    "print(\"N_y:\",n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Initalize Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.rand(n_x,n_h) # input-hidden layer arasındaki agırlıklar w[0] -> Mesela, ilk noronun agırlıkları \n",
    "W2 = np.random.rand(n_h,n_y) # hidden_layer ile output_layer arasındaki agirliklar\n",
    "b1 = np.random.rand(n_h,1)\n",
    "b2 = np.random.rand(n_y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:\n",
      "[[0.85231319 0.52837607 0.5705212  0.09916507 0.59132746]\n",
      " [0.57097091 0.37039451 0.90022406 0.15587152 0.74320266]\n",
      " [0.74285507 0.88793231 0.48940715 0.15738595 0.95094928]\n",
      " [0.79941413 0.15994955 0.7172062  0.3632363  0.22249157]]\t W1.shape:(4, 5)\n",
      "b1:\n",
      "[[0.36117809]\n",
      " [0.43983447]\n",
      " [0.5369493 ]\n",
      " [0.04459745]\n",
      " [0.60337236]]\t b1.shape:(4, 5)\n",
      "W2:\n",
      "[[0.77974584 0.99945576 0.35497081]\n",
      " [0.09326779 0.45437124 0.42604761]\n",
      " [0.66135956 0.12115397 0.66520138]\n",
      " [0.15775684 0.28489648 0.91731744]\n",
      " [0.79430416 0.99135754 0.78633163]]\t W2.shape:(5, 3)\n",
      "b2:\n",
      "[[0.83540884]\n",
      " [0.64366388]\n",
      " [0.17261434]]\t b2.shape:(3, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"W1:\\n{W1}\\t W1.shape:{W1.shape}\")\n",
    "print(f\"b1:\\n{b1}\\t b1.shape:{W1.shape}\")\n",
    "print(f\"W2:\\n{W2}\\t W2.shape:{W2.shape}\")\n",
    "print(f\"b2:\\n{b2}\\t b2.shape:{b2.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Lineer Calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineer_fonk(X,W,b):\n",
    "    if X.shape[0] == (4,1): # Check x shape is (4,1) or not \n",
    "        pass\n",
    "    else:\n",
    "        X = X.reshape(-1,1)\n",
    "    \n",
    "    assert X.shape == (X.shape[0],1)\n",
    "    \n",
    "    return np.dot(W.T,X)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b718af85f5ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msample_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msample_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\\ty:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X.shape:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\\ty.shape:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "sample_x = Xtrain[:,1]\n",
    "sample_y = ytrain[1]\n",
    "print(\"X:\",sample_x,\"\\ty:\",sample_y)\n",
    "print(\"X.shape:\",sample_x.shape,\"\\ty.shape:\",sample_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X.shape:\",sample_x.shape,\"\\ty.shape:\",sample_y.shape)\n",
    "print(\"W.shape:\",W1.shape,\"\\tb.shape:\",b1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = lineer_fonk(sample_x,W1,b1)\n",
    "print(\"Lineer Func.'s Result(Z1):\\n\",Z1,\"\\t Z1.shape:\",Z1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Non-lineer Calculation with Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "def softmax(Z):\n",
    "    score = np.exp(Z) / np.sum(np.exp(Z))  \n",
    "    return score \n",
    "\n",
    "def tanh(Z):\n",
    "    score = np.tanh(Z)\n",
    "    return score \n",
    "\n",
    "def sigmoid(Z):\n",
    "    score = 1 / (1 + np.exp(-Z))\n",
    "    return score \n",
    "\n",
    "# Non-lineer part of a neuron \n",
    "def non_lineer(Z,f='sigmoid'):\n",
    "    if f == 'sigmoid':\n",
    "        return sigmoid(Z)\n",
    "    elif f == 'softmax': \n",
    "        return softmax(Z)\n",
    "    elif f== 'tanh':\n",
    "        return tanh(Z)\n",
    "    else:\n",
    "        raise Exception('Wrong Activation Function in Non-lineer!')\n",
    "\n",
    "# Derivatives of activation functions for Backprop-step \n",
    "def sigmoid_derivative(X):\n",
    "    derivative  =  X * (1 - X)\n",
    "    return derivative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = non_lineer(Z1,f='tanh')\n",
    "print(\"Before Activation Function(Z1):\\n\",Z1,\"\\t Shape:\",Z1.shape)\n",
    "print(\"\\nAfter Activation Function(A1):\\n\",A1,\"\\t Shape:\",A1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now A is our new input for next hidden layer. \n",
    "Z2 = lineer_fonk(A1,W2,b2)\n",
    "print(\"Lineer Func.'s Result(Z2):\\n\",Z2,\"\\t Z2.shape:\",Z2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = non_lineer(Z2,f='softmax')\n",
    "print(\"Before Activation Function(Z2):\\n\",Z2,\"\\t Shape:\",Z2.shape)\n",
    "print(\"\\nAfter Activation Function(A2):\\n\",A2,\"\\t Shape:\",A2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = A2 # It's last output in our nn for 1 forward prop step.\n",
    "print(\"Forward Propagation Last Result (PREDICTIONs):\\n\",predictions,\"\\t Shape: \",predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction is \",np.argmax(predictions),\".Its mean is : \",idx_to_target[np.argmax(predictions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Calculate Loss & Cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def cross_entropy(self,y,yhat):\n",
    "    ce = np.multiply(np.log(yhat),y) + np.multiply(np.log(1-yhat),1-y)\n",
    "    return -ce \n",
    "    \n",
    "def calc_cost(losses):\n",
    "    return np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lets remember the sample_y and predictions...\\n\")\n",
    "print(\"Sample_y:\",sample_y,\"Prediction:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss = cross_entropy(sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "losses = [] # intialize loss array \n",
    "init_params() # initialize weights & biases \n",
    "for epoch in epochs: \n",
    "    yhat,_ = forward_propagation(X) # predict something in case of current weights & biases \n",
    "    loss = cross_entropy(y,yhat) # calculate loss \n",
    "    backward_propagation() # Calculate gradients \n",
    "    update_params() # Update parameters\n",
    "    losses.append(loss) # add loss \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        cost = calc_cost(losses) # calculate cost \n",
    "        print(\"Epoch {} \\t ---> \\t Cost: \".format(epoch,cost)) # Show cost for every 10 epoch\n",
    "   ''' \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,layers=[4,5,3],lr=0.001,iterations=100):\n",
    "        '''\n",
    "        layers[0] = input layer size\n",
    "        layers[1] = hidden layer size\n",
    "        layers[2] = output layer size\n",
    "        lr = learning rate\n",
    "        iterations = epochs \n",
    "        params = a dict which will contain Weights and Biases \n",
    "        loss = a list which will contain losses\n",
    "        X  = inputs/features\n",
    "        y = outputs/targets \n",
    "        '''\n",
    "        self.params = {}\n",
    "        self.learning_rate = lr\n",
    "        self.iterations = iterations # epoch \n",
    "        self.loss = []\n",
    "        self.sample_size = None\n",
    "        self.layers = layers\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "    def init_params(self):\n",
    "        '''\n",
    "        n_x : size of input layer\n",
    "        n_h : size of hidden layer\n",
    "        n_y : size of output layer\n",
    "        w1  : Weights of input layer to hidden layer\n",
    "        b1  : biases of input layer to hidden layer\n",
    "        w2  : Weights of hidden layer to output layer\n",
    "        b2  : biases of hidden layer to output layer \n",
    "        '''\n",
    "        self.n_x,self.n_h,self.n_y = self.layers[0],self.layers[1],self.layers[2]\n",
    "        self.w1 = np.random.rand(self.n_h,self.n_x)\n",
    "        self.b1 = np.random.rand(self.n_h,1)\n",
    "        self.w2 = np.random.rand(self.n_y,self.n_h)\n",
    "        self.b2 = np.random.rand(self.n_y,1)\n",
    "        \n",
    "        assert self.w1.shape == (self.n_h,self.n_x)\n",
    "        assert self.w2.shape == (self.n_y,self.n_h)\n",
    "        assert self.b1.shape == (self.n_h,1)\n",
    "        assert self.b2.shape == (self.n_y,1)\n",
    "        self.params = {\"w1\":self.w1,\"b1\":self.b1,\"w2\":self.w2,\"b2\":self.b2}\n",
    "    def lineer_func(self,X,W,b):\n",
    "        if X.shape != (X.shape[0],1):\n",
    "            X = X.reshape(-1,1)\n",
    "        return np.dot(W,X) + b \n",
    "\n",
    "    def non_lineer_func(self,Z,f='sigmoid'):\n",
    "        '''\n",
    "        Z : lineer calculation result \n",
    "        f : activation function \n",
    "        '''\n",
    "        if f == 'sigmoid':\n",
    "            score = 1 / (1 + np.exp(-Z))\n",
    "            return score\n",
    "        elif f == 'softmax': \n",
    "            score = np.exp(Z) / np.sum(np.exp(Z))  \n",
    "            return score \n",
    "        elif f== 'tanh':\n",
    "            score = np.tanh(Z)\n",
    "            return score \n",
    "        else:\n",
    "            raise Exception('Wrong Activation Function in Non-lineer!')\n",
    "    \n",
    "    def forward_propagation(self,X,y):\n",
    "        W1 = self.params[\"w1\"]\n",
    "        b1 = self.params[\"b1\"]\n",
    "        W2 = self.params[\"w2\"]\n",
    "        b2 = self.params[\"b2\"]\n",
    "        \n",
    "        assert len(y) == 1 \n",
    "        assert W1.shape[1] ==  X.shape[0]\n",
    "        \n",
    "        Z1 = lineer_func(X,W1,b1)\n",
    "        A1 = non_lineer_func(Z1,f='tanh')\n",
    "\n",
    "        assert W2.T.shape[1] == A1.shape[0]\n",
    "\n",
    "        Z2 = lineer_func(A1,W2,b2)\n",
    "        yhat = non_lineer_func(Z2,f='softmax')\n",
    "\n",
    "        loss = np.multiply(np.log(yhat),y) + np.multiply(np.log(1-yhat),1-y)\n",
    "\n",
    "        self.cache = {\"Z1\":Z1,\"A1\":A1,\"Z2\":Z2,\"yhat\":yhat}\n",
    "\n",
    "        return yhat,loss\n",
    "    \n",
    "    def tanh_derivative(A):\n",
    "        '''\n",
    "        Args: You need to give Ai to find derivate of Zi\n",
    "        '''\n",
    "        return (1- np.power(A,2))\n",
    "    \n",
    "    def sigmoid_derivative(A):\n",
    "        '''\n",
    "        Args: You need to give Ai to find derivate of Zi\n",
    "        '''\n",
    "        return np.multiply(A,(1-A))\n",
    "    \n",
    "    def back_propagation(self,X,y,params,cache):\n",
    "        '''\n",
    "        compute derivative and update weights&biases\n",
    "        '''\n",
    "\n",
    "        m = self.sample_size\n",
    "        W1 = self.params[\"w1\"]\n",
    "        W2 = self.params[\"w2\"]\n",
    "        \n",
    "        A1 = cache[\"A1\"]\n",
    "        yhat = cache[\"yhat\"]\n",
    "        m = 120 \n",
    "        assert len(y) == 1 \n",
    "        assert yhat.shape == (self.n_y,1)\n",
    "        # backward\n",
    "        dz2 =  yhat - y \n",
    "        print(\"dZ2:\\n\",dz2,\"\\t Shape:\",dz2.shape)\n",
    "        print(\"A1:\\n\",A1,\"\\t Shape:\",A1.shape)\n",
    "        print(\"m:\",m)\n",
    "        dw2 = np.dot(dz2,A1.T) / m\n",
    "        db2 = np.sum(dz2,axis=1,keepdims=True) / m\n",
    "\n",
    "        dz1 = np.multiply(np.dot(W2.T,dz2), tanh_derivative(A1))\n",
    "\n",
    "        dw1 = np.dot(dz1,X.T) / m \n",
    "        db1 = np.sum(dz1,axis=1,keepdims=True) / m\n",
    "        \n",
    "        self.grads = {\"dw1\":dw1,\"db1\":db1,\"dw2\":dw2,\"db2\":db2}\n",
    "        return self.grads\n",
    "    \n",
    "    def update_params(self,params,grads):\n",
    "        lr = self.learning_rate\n",
    "        # Weight and biases \n",
    "        W1 = self.params[\"w1\"]\n",
    "        b1 = self.params[\"b1\"]\n",
    "        W2 = self.params[\"w2\"]\n",
    "        b2 = self.params[\"b2\"]\n",
    "\n",
    "        # Gradients\n",
    "        dw1 = self.grads[\"dw1\"]\n",
    "        db1 = self.grads[\"db1\"]\n",
    "        dw2 = self.grads[\"dw2\"]\n",
    "        db2 = self.grads[\"db2\"]\n",
    "\n",
    "        # Update parameters according to given parameters and learning rate \n",
    "        w1 = W1 - lr * dw1\n",
    "        b1 = b1 - lr * db1\n",
    "        w2 = W2 - lr * dw2\n",
    "        b2 = b2 - lr * db2\n",
    "\n",
    "        self.params = {\"w1\":w1,\"b1\":b1,\"w2\":w2,\"b2\":b2}\n",
    "        return self.params\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        print(\"INFO: Training is began...\")\n",
    "        #trainable_count = len(self.params[\"w1\"].reshape(-1)) + len(self.params[\"b1\"].reshape(-1)) + len(self.params[\"w2\"].reshape(-1)) + len(self.params[\"b2\"].reshape(-1))\n",
    "        #print(f\"INFO: Layer Count:{len(self.layers)-1}\\n Learning Rate:{self.learning_rate}\\n Epochs:{self.iterations} \\nTrainable Parameters:{trainable_count}\")\n",
    "        self.init_params()\n",
    "        self.sample_size = X.shape[1]\n",
    "        for epoch in range(self.iterations):\n",
    "            for i in range(self.sample_size):\n",
    "                self.X = X[:,i].reshape(-1,1)\n",
    "                self.y = y[i].reshape(-1,1)\n",
    "                yhat,loss = self.forward_propagation()\n",
    "                self.back_propagation(yhat)\n",
    "                self.update_params()\n",
    "\n",
    "                self.losses.append(loss)\n",
    "            cost = -1 * np.mean(self.losses)\n",
    "            cost = float(np.squeeze(cost)) # be sure to 1 dimension\n",
    "            assert isinstance(cost,float)\n",
    "            \n",
    "            losses = [] # clear losses list \n",
    "            if epoch % 10 == 0:\n",
    "                print(\"Epoch {} \\t ---> \\t Cost: \".format(epoch+1,cost)) # Show cost for every 10 epoch\n",
    "\n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        Makes prediction using model on test data\n",
    "        '''\n",
    "        yhat,_ = self.forward_propagation()\n",
    "        return yhat\n",
    "    \n",
    "    def acc(self,y,yhat):\n",
    "        acc = int( sum(y==yhat) / len(y) * 100)\n",
    "        return acc \n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.loss)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylable(\"logloss\")\n",
    "        plt.title(\"Loss cure for training\")\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain.shape: (4, 120) \tytrain.shape: (1, 120)\n"
     ]
    }
   ],
   "source": [
    "print(\"Xtrain.shape:\",Xtrain.shape,\"\\tytrain.shape:\",ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': array([[0.14899151, 0.18733881, 0.90677298, 0.88880441],\n",
       "        [0.8833128 , 0.67265856, 0.18755734, 0.41234908],\n",
       "        [0.94058584, 0.47771412, 0.7160983 , 0.57963805],\n",
       "        [0.74424959, 0.38327416, 0.45494457, 0.56430957],\n",
       "        [0.41926703, 0.69762027, 0.67472237, 0.97999816]]),\n",
       " 'b1': array([[0.02494273],\n",
       "        [0.40873935],\n",
       "        [0.0643523 ],\n",
       "        [0.87553245],\n",
       "        [0.21793623]]),\n",
       " 'w2': array([[0.68916411, 0.80049042, 0.7353547 , 0.85115575, 0.4925957 ],\n",
       "        [0.31059814, 0.78671409, 0.04729197, 0.15863713, 0.0520535 ],\n",
       "        [0.466365  , 0.83268637, 0.89182753, 0.71515263, 0.50165694]]),\n",
       " 'b2': array([[0.36734346],\n",
       "        [0.12041287],\n",
       "        [0.51570921]])}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_.shape:(4, 1)\ty_.shape:(1, 1)\n"
     ]
    }
   ],
   "source": [
    "X_,y_ = Xtrain[:,0].reshape(-1,1),ytrain[:,0].reshape(-1,1)\n",
    "print(f\"X_.shape:{X_.shape}\\ty_.shape:{y_.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lineer_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-196-95c199c301f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-189-afa3b28ed87a>\u001b[0m in \u001b[0;36mforward_propagation\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m  \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mZ1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlineer_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0mA1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnon_lineer_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tanh'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lineer_func' is not defined"
     ]
    }
   ],
   "source": [
    "nn.forward_propagation(X_,y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = nn.params[\"b1\"]\n",
    "W1 = nn.params[\"w1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = nn.lineer_func(X_,W1,b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = nn.non_lineer_func(Z1,f='tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = nn.params[\"w2\"]\n",
    "b2 = nn.params[\"b2\"]\n",
    "Z2 = nn.lineer_func(A1,W2,b2)\n",
    "A2 = nn.non_lineer_func(Z2,f='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A2:\n",
      " [[0.48209349]\n",
      " [0.04126481]\n",
      " [0.4766417 ]] \t Shape: (3, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"A2:\\n\",A2,\"\\t Shape:\",A2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {\"Z1\":Z1,\"A1\":A1,\"Z2\":Z2,\"yhat\":A2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dZ2:\n",
      " [[0.48209349]\n",
      " [0.04126481]\n",
      " [0.4766417 ]] \t Shape: (3, 1)\n",
      "A1:\n",
      " [[0.99401515]\n",
      " [0.99999992]\n",
      " [0.9999999 ]\n",
      " [0.99999923]\n",
      " [0.99999496]] \t Shape: (5, 1)\n",
      "m: 120\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tanh_derivative' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-203-e595e06e9d62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-189-afa3b28ed87a>\u001b[0m in \u001b[0;36mback_propagation\u001b[1;34m(self, X, y, params, cache)\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mdb2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdz2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mdz1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdz2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtanh_derivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mdw1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdz1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tanh_derivative' is not defined"
     ]
    }
   ],
   "source": [
    "grads = nn.back_propagation(X_,y_,nn.params,cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.params[\"w1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.cache[\"Z2\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRUCTURE OF ASSIGNMENT \n",
    "'''\n",
    "losses = [] # intialize loss array \n",
    "init_params() # initialize weights & biases \n",
    "for epoch in epochs: \n",
    "    yhat,_ = forward_propagation(X) # predict something in case of current weights & biases \n",
    "    loss = cross_entropy(y,yhat) # calculate loss \n",
    "    backward_propagation() # Calculate gradients \n",
    "    update_params() # Update parameters\n",
    "    losses.append(loss) # add loss \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        cost = calc_cost(losses) # calculate cost \n",
    "        print(\"Epoch {} \\t ---> \\t Cost: \".format(epoch,cost)) # Show cost for every 10 epoch\n",
    "''' \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layers):\n",
    "        '''\n",
    "        n_x : size of input layer\n",
    "        n_h : size of hidden layer\n",
    "        n_y : size of output layer\n",
    "        w1  : Weights of input layer to hidden layer\n",
    "        b1  : biases of input layer to hidden layer\n",
    "        w2  : Weights of hidden layer to output layer\n",
    "        b2  : biases of hidden layer to output layer \n",
    "        '''\n",
    "        n_x,n_h,n_y = layers[0],layers[1],layers[2]\n",
    "        W1 = np.random.rand(n_h,n_x)\n",
    "        b1 = np.random.rand(n_h,1)\n",
    "        W2 = np.random.rand(n_y,n_h)\n",
    "        b2 = np.random.rand(n_y,1)\n",
    "        params = {\"w1\":W1,\"b1\":b1,\"w2\":W2,\"b2\":b2}\n",
    "        return params\n",
    "    \n",
    "def lineer_func(X,W,b):\n",
    "    if X.shape != (X.shape[0],1):\n",
    "        X = X.reshape(-1,1)\n",
    "    return np.dot(W,X) + b \n",
    "\n",
    "def non_lineer_func(Z,f='sigmoid'):\n",
    "    '''\n",
    "    Z : lineer calculation result \n",
    "    f : activation function \n",
    "    '''\n",
    "    if f == 'sigmoid':\n",
    "        score = 1 / (1 + np.exp(-Z))\n",
    "        return score\n",
    "    elif f == 'softmax': \n",
    "        score = np.exp(Z) / np.sum(np.exp(Z))  \n",
    "        return score \n",
    "    elif f== 'tanh':\n",
    "        score = np.tanh(Z)\n",
    "        return score \n",
    "    else:\n",
    "        raise Exception('Wrong Activation Function in Non-lineer!')\n",
    "\n",
    "def forward_propagation(X,y,params):\n",
    "    '''\n",
    "    Args: X: input\n",
    "          y: real value\n",
    "          params: weights and biases\n",
    "    Return: \n",
    "          loss: loss\n",
    "          cache: calculation results z1,a1,z2,a2\n",
    "    '''\n",
    "    W1 = params[\"w1\"]\n",
    "    b1 = params[\"b1\"]\n",
    "    W2 = params[\"w2\"]\n",
    "    b2 = params[\"b2\"]\n",
    "\n",
    "    assert len(y) == 1 \n",
    "    assert W1.shape[1] ==  X.shape[0]\n",
    "    m = 120 # sample_size\n",
    "    Z1 = lineer_func(X,W1,b1)\n",
    "    A1 = non_lineer_func(Z1,f='tanh')\n",
    "    print(\"A1.shape:\",A1.shape,\"W2.shape:\",W2.shape)\n",
    "    assert W2.shape[1] == A1.shape[0]\n",
    "\n",
    "    Z2 = lineer_func(A1,W2,b2)\n",
    "    yhat = non_lineer_func(Z2,f='softmax')\n",
    "\n",
    "    loss = -1 * (np.multiply(np.log(yhat),y) + np.multiply(np.log(1-yhat),1-y))\n",
    "    cost = np.sum(loss) / m \n",
    "    cache = {\"Z1\":Z1,\"A1\":A1,\"Z2\":Z2,\"yhat\":yhat}\n",
    "\n",
    "    return cost,cache\n",
    "\n",
    "def tanh_derivative(A):\n",
    "    '''\n",
    "    Args: You need to give Ai to find derivate of Zi\n",
    "    '''\n",
    "    return (1- np.power(A,2))\n",
    "\n",
    "def back_propagation(X,y,params,cache):\n",
    "    '''\n",
    "    compute derivative and update weights&biases\n",
    "    Returns gradients(grads)\n",
    "    '''\n",
    "\n",
    "    m = 120 # sample_size\n",
    "    W1 = params[\"w1\"]\n",
    "    W2 = params[\"w2\"]\n",
    "\n",
    "    A1 = cache[\"A1\"]\n",
    "    yhat = cache[\"yhat\"]\n",
    "    m = 120 \n",
    "    assert len(y) == 1 \n",
    "    assert yhat.shape == (3,1) # n_y,1\n",
    "    # backward\n",
    "    dz2 =  yhat - y \n",
    "    #print(\"dZ2:\\n\",dz2,\"\\t Shape:\",dz2.shape)\n",
    "    #print(\"A1:\\n\",A1,\"\\t Shape:\",A1.shape)\n",
    "    #print(\"m:\",m)\n",
    "    dw2 = np.dot(dz2,A1.T) / m\n",
    "    db2 = np.sum(dz2,axis=1,keepdims=True) / m\n",
    "\n",
    "    dz1 = np.multiply(np.dot(W2.T,dz2), tanh_derivative(A1))\n",
    "\n",
    "    dw1 = np.dot(dz1,X.T) / m \n",
    "    db1 = np.sum(dz1,axis=1,keepdims=True) / m\n",
    "\n",
    "    grads = {\"dw1\":dw1,\"db1\":db1,\"dw2\":dw2,\"db2\":db2}\n",
    "    return grads\n",
    "\n",
    "def update_params(params,grads,lr=0.001):\n",
    "    '''\n",
    "    Returns updated params\n",
    "    '''\n",
    "    # Weight and biases \n",
    "    W1 = params[\"w1\"]\n",
    "    b1 = params[\"b1\"]\n",
    "    W2 = params[\"w2\"]\n",
    "    b2 = params[\"b2\"]\n",
    "\n",
    "    # Gradients\n",
    "    dw1 = grads[\"dw1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dw2 = grads[\"dw2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "\n",
    "    # Update parameters according to given parameters and learning rate \n",
    "    w1 = W1 - lr * dw1\n",
    "    b1 = b1 - lr * db1\n",
    "    w2 = W2 - lr * dw2\n",
    "    b2 = b2 - lr * db2\n",
    "\n",
    "    params = {\"w1\":w1,\"b1\":b1,\"w2\":w2,\"b2\":b2}\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_.shape:(4, 1)\ty_.shape:(1, 1)\n",
      "params\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'w1': array([[0.35734249, 0.97846537, 0.36536034, 0.27000946],\n",
       "        [0.29420431, 0.70846036, 0.07207911, 0.04133522],\n",
       "        [0.56514474, 0.2613164 , 0.00825787, 0.32579672],\n",
       "        [0.93920099, 0.3159115 , 0.07804344, 0.5159972 ],\n",
       "        [0.9835994 , 0.41844141, 0.94191881, 0.35488166]]),\n",
       " 'b1': array([[0.29855978],\n",
       "        [0.17931056],\n",
       "        [0.37230165],\n",
       "        [0.31154832],\n",
       "        [0.65791015]]),\n",
       " 'w2': array([[0.20570394, 0.63976103, 0.74851606, 0.34773297, 0.10339065],\n",
       "        [0.81324585, 0.23714137, 0.23137034, 0.98826681, 0.87490822],\n",
       "        [0.35446091, 0.87557007, 0.37599981, 0.28597489, 0.0226546 ]]),\n",
       " 'b2': array([[0.73329502],\n",
       "        [0.74817506],\n",
       "        [0.85913364]])}"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [4,5,3]\n",
    "lr = 0.001\n",
    "X1,y1 = Xtrain[:,2].reshape(-1,1),ytrain[:,3].reshape(-1,1)\n",
    "print(f\"X_.shape:{X_.shape}\\ty_.shape:{y_.shape}\")\n",
    "params = init_params(layers)\n",
    "print(\"params\\n\")\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1.shape: (5, 1) W2.shape: (3, 5)\n"
     ]
    }
   ],
   "source": [
    "cost,cache = forward_propagation(X1,y1,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031204988925457193"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Z1': array([[ 8.3374313 ],\n",
       "        [ 4.88157852],\n",
       "        [ 5.81064684],\n",
       "        [ 9.27638955],\n",
       "        [14.57352743]]),\n",
       " 'A1': array([[0.99999989],\n",
       "        [0.99988494],\n",
       "        [0.99998205],\n",
       "        [0.99999998],\n",
       "        [1.        ]]),\n",
       " 'Z2': array([[2.77831259],\n",
       "        [3.89307611],\n",
       "        [2.77368639]]),\n",
       " 'yhat': array([[0.19824625],\n",
       "        [0.60442251],\n",
       "        [0.19733124]])}"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = back_propagation(X1,y1,params,cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dw1': array([[-1.01639005e-08, -4.56639010e-09, -7.95435694e-09,\n",
       "         -3.09336103e-09],\n",
       "        [-1.73264078e-05, -7.78432815e-06, -1.35597974e-05,\n",
       "         -5.27325455e-06],\n",
       "        [-2.05023661e-06, -9.21120796e-07, -1.60453300e-06,\n",
       "         -6.23985055e-07],\n",
       "        [-1.81238712e-09, -8.14260882e-10, -1.41838992e-09,\n",
       "         -5.51596081e-10],\n",
       "        [-2.25859797e-14, -1.01473242e-14, -1.76759841e-14,\n",
       "         -6.87399382e-15]]),\n",
       " 'db1': array([[-1.47302906e-09],\n",
       "        [-2.51107360e-06],\n",
       "        [-2.97135741e-07],\n",
       "        [-2.62664801e-10],\n",
       "        [-3.27333039e-15]]),\n",
       " 'dw2': array([[-0.00668128, -0.00668051, -0.00668116, -0.00668128, -0.00668128],\n",
       "        [-0.00329648, -0.0032961 , -0.00329642, -0.00329648, -0.00329648],\n",
       "        [-0.00668891, -0.00668814, -0.00668879, -0.00668891, -0.00668891]]),\n",
       " 'db2': array([[-0.00668128],\n",
       "        [-0.00329648],\n",
       "        [-0.00668891]])}"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = update_params(params,grads,lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': array([[0.35734249, 0.97846537, 0.36536034, 0.27000946],\n",
       "        [0.29420604, 0.70846114, 0.07208046, 0.04133574],\n",
       "        [0.56514495, 0.26131649, 0.00825803, 0.32579679],\n",
       "        [0.93920099, 0.3159115 , 0.07804344, 0.5159972 ],\n",
       "        [0.9835994 , 0.41844141, 0.94191881, 0.35488166]]),\n",
       " 'b1': array([[0.29855978],\n",
       "        [0.17931081],\n",
       "        [0.37230168],\n",
       "        [0.31154832],\n",
       "        [0.65791015]]),\n",
       " 'w2': array([[0.20637207, 0.64042908, 0.74918418, 0.3484011 , 0.10405877],\n",
       "        [0.8135755 , 0.23747098, 0.23169999, 0.98859646, 0.87523787],\n",
       "        [0.3551298 , 0.87623889, 0.37666869, 0.28664378, 0.02332349]]),\n",
       " 'b2': array([[0.73396315],\n",
       "        [0.74850471],\n",
       "        [0.85980253]])}"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

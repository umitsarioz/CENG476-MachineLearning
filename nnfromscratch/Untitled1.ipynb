{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_ds = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (150, 4) y.shape: (150,)\n"
     ]
    }
   ],
   "source": [
    "X,y = iris_ds.data,iris_ds.target\n",
    "print(\"X.shape:\",X.shape,\"y.shape:\",y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X,y,test_size=.2,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain.shape: (120, 4) ytrain.shape: (120,)\n",
      "\n",
      "Xtrain[:5]:\n",
      "[[5.8 4.  1.2 0.2]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [4.4 2.9 1.4 0.2]]\n",
      "\n",
      "ytrain[:5]:\n",
      "[0 0 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Xtrain.shape:\",Xtrain.shape,\"ytrain.shape:\",ytrain.shape)\n",
    "print(f\"\\nXtrain[:5]:\\n{Xtrain[:5]}\\n\\nytrain[:5]:\\n{ytrain[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain,Xtest,ytrain,ytest = Xtrain.T,Xtest.T,ytrain.reshape(-1,1).T,ytest.reshape(-1,1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain.shape: (4, 120) ytrain.shape: (1, 120)\n",
      "\n",
      "Xtrain[:5]:\n",
      "[[5.8 4.8 6.9 6.6 4.4]\n",
      " [4.  3.4 3.1 2.9 2.9]\n",
      " [1.2 1.9 5.4 4.6 1.4]\n",
      " [0.2 0.2 2.1 1.3 0.2]]\n",
      "\n",
      "ytrain[:5]:\n",
      "[[0 0 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Xtrain.shape:\",Xtrain.shape,\"ytrain.shape:\",ytrain.shape)\n",
    "print(f\"\\nXtrain[:5]:\\n{Xtrain[:,:5]}\\n\\nytrain[:5]:\\n{ytrain[:,:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n"
     ]
    }
   ],
   "source": [
    "idx_to_target = {}\n",
    "for idx,target in enumerate(iris_ds.target_names):\n",
    "    idx_to_target[idx] = target\n",
    "print(idx_to_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Layer Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_X: 4\n",
      "N_h: 5\n",
      "N_y: 3\n"
     ]
    }
   ],
   "source": [
    "n_h = 5  # hidden_layer_units\n",
    "n_x = len(Xtrain[:,0]) # input_layer_units \n",
    "n_y = len(idx_to_target) # output_layer_units \n",
    "\n",
    "print(\"N_X:\",n_x)\n",
    "print(\"N_h:\",n_h)\n",
    "print(\"N_y:\",n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Initalize Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = np.random.rand(n_x,n_h) # input-hidden layer arasındaki agırlıklar w[0] -> Mesela, ilk noronun agırlıkları \n",
    "W2 = np.random.rand(n_h,n_y) # hidden_layer ile output_layer arasındaki agirliklar\n",
    "b1 = np.random.rand(n_h,1)\n",
    "b2 = np.random.rand(n_y,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:\n",
      "[[0.43660609 0.60542122 0.63645407 0.79411884 0.95800011]\n",
      " [0.81065925 0.79752391 0.94499652 0.76905032 0.52645312]\n",
      " [0.97103014 0.15069328 0.25463608 0.74808575 0.27653712]\n",
      " [0.96842085 0.041185   0.56537396 0.47554876 0.45363784]]\t W1.shape:(4, 5)\n",
      "b1:\n",
      "[[0.63734679]\n",
      " [0.91059013]\n",
      " [0.35794682]\n",
      " [0.94310387]\n",
      " [0.54065691]]\t b1.shape:(4, 5)\n",
      "W2:\n",
      "[[0.51190471 0.6132555  0.61365144]\n",
      " [0.35991317 0.73489262 0.51674482]\n",
      " [0.97418776 0.90291177 0.17056371]\n",
      " [0.22560971 0.95037523 0.94746135]\n",
      " [0.15798704 0.86102936 0.83883711]]\t W2.shape:(5, 3)\n",
      "b2:\n",
      "[[0.08515251]\n",
      " [0.20459528]\n",
      " [0.47954159]]\t b2.shape:(3, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f\"W1:\\n{W1}\\t W1.shape:{W1.shape}\")\n",
    "print(f\"b1:\\n{b1}\\t b1.shape:{W1.shape}\")\n",
    "print(f\"W2:\\n{W2}\\t W2.shape:{W2.shape}\")\n",
    "print(f\"b2:\\n{b2}\\t b2.shape:{b2.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Lineer Calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lineer_fonk(X,W,b):\n",
    "    if X.shape[0] == (4,1): # Check x shape is (4,1) or not \n",
    "        pass\n",
    "    else:\n",
    "        X = X.reshape(-1,1)\n",
    "    \n",
    "    assert X.shape == (X.shape[0],1)\n",
    "    \n",
    "    return np.dot(W.T,X)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_x = Xtrain[:,1]\n",
    "sample_y = ytrain[1]\n",
    "print(\"X:\",sample_x,\"\\ty:\",sample_y)\n",
    "print(\"X.shape:\",sample_x.shape,\"\\ty.shape:\",sample_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X.shape:\",sample_x.shape,\"\\ty.shape:\",sample_y.shape)\n",
    "print(\"W.shape:\",W1.shape,\"\\tb.shape:\",b1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = lineer_fonk(sample_x,W1,b1)\n",
    "print(\"Lineer Func.'s Result(Z1):\\n\",Z1,\"\\t Z1.shape:\",Z1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Non-lineer Calculation with Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "def softmax(Z):\n",
    "    score = np.exp(Z) / np.sum(np.exp(Z))  \n",
    "    return score \n",
    "\n",
    "def tanh(Z):\n",
    "    score = np.tanh(Z)\n",
    "    return score \n",
    "\n",
    "def sigmoid(Z):\n",
    "    score = 1 / (1 + np.exp(-Z))\n",
    "    return score \n",
    "\n",
    "# Non-lineer part of a neuron \n",
    "def non_lineer(Z,f='sigmoid'):\n",
    "    if f == 'sigmoid':\n",
    "        return sigmoid(Z)\n",
    "    elif f == 'softmax': \n",
    "        return softmax(Z)\n",
    "    elif f== 'tanh':\n",
    "        return tanh(Z)\n",
    "    else:\n",
    "        raise Exception('Wrong Activation Function in Non-lineer!')\n",
    "\n",
    "# Derivatives of activation functions for Backprop-step \n",
    "def sigmoid_derivative(X):\n",
    "    derivative  =  X * (1 - X)\n",
    "    return derivative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = non_lineer(Z1,f='tanh')\n",
    "print(\"Before Activation Function(Z1):\\n\",Z1,\"\\t Shape:\",Z1.shape)\n",
    "print(\"\\nAfter Activation Function(A1):\\n\",A1,\"\\t Shape:\",A1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now A is our new input for next hidden layer. \n",
    "Z2 = lineer_fonk(A1,W2,b2)\n",
    "print(\"Lineer Func.'s Result(Z2):\\n\",Z2,\"\\t Z2.shape:\",Z2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = non_lineer(Z2,f='softmax')\n",
    "print(\"Before Activation Function(Z2):\\n\",Z2,\"\\t Shape:\",Z2.shape)\n",
    "print(\"\\nAfter Activation Function(A2):\\n\",A2,\"\\t Shape:\",A2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = A2 # It's last output in our nn for 1 forward prop step.\n",
    "print(\"Forward Propagation Last Result (PREDICTIONs):\\n\",predictions,\"\\t Shape: \",predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prediction is \",np.argmax(predictions),\".Its mean is : \",idx_to_target[np.argmax(predictions)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Calculate Loss & Cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def cross_entropy(self,y,yhat):\n",
    "    ce = np.multiply(np.log(yhat),y) + np.multiply(np.log(1-yhat),1-y)\n",
    "    return -ce \n",
    "    \n",
    "def calc_cost(losses):\n",
    "    return np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lets remember the sample_y and predictions...\\n\")\n",
    "print(\"Sample_y:\",sample_y,\"Prediction:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss = cross_entropy(sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "losses = [] # intialize loss array \n",
    "init_params() # initialize weights & biases \n",
    "for epoch in epochs: \n",
    "    yhat,_ = forward_propagation(X) # predict something in case of current weights & biases \n",
    "    loss = cross_entropy(y,yhat) # calculate loss \n",
    "    backward_propagation() # Calculate gradients \n",
    "    update_params() # Update parameters\n",
    "    losses.append(loss) # add loss \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        cost = calc_cost(losses) # calculate cost \n",
    "        print(\"Epoch {} \\t ---> \\t Cost: \".format(epoch,cost)) # Show cost for every 10 epoch\n",
    "   ''' \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,layers=[4,5,3],lr=0.001,iterations=100):\n",
    "        '''\n",
    "        layers[0] = input layer size\n",
    "        layers[1] = hidden layer size\n",
    "        layers[2] = output layer size\n",
    "        lr = learning rate\n",
    "        iterations = epochs \n",
    "        params = a dict which will contain Weights and Biases \n",
    "        loss = a list which will contain losses\n",
    "        X  = inputs/features\n",
    "        y = outputs/targets \n",
    "        '''\n",
    "        self.params = {}\n",
    "        self.learning_rate = lr\n",
    "        self.iterations = iterations # epoch \n",
    "        self.loss = []\n",
    "        self.sample_size = None\n",
    "        self.layers = layers\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "\n",
    "\n",
    "    def init_params(self):\n",
    "        '''\n",
    "        n_x : size of input layer\n",
    "        n_h : size of hidden layer\n",
    "        n_y : size of output layer\n",
    "        w1  : Weights of input layer to hidden layer\n",
    "        b1  : biases of input layer to hidden layer\n",
    "        w2  : Weights of hidden layer to output layer\n",
    "        b2  : biases of hidden layer to output layer \n",
    "        '''\n",
    "        self.n_x,self.n_h,self.n_y = self.layers[0],self.layers[1],self.layers[2]\n",
    "        self.w1 = np.random.rand(self.n_h,self.n_x)\n",
    "        self.b1 = np.random.rand(self.n_h,1)\n",
    "        self.w2 = np.random.rand(self.n_y,self.n_h)\n",
    "        self.b2 = np.random.rand(self.n_y,1)\n",
    "        \n",
    "        assert self.w1.shape == (self.n_h,self.n_x)\n",
    "        assert self.w2.shape == (self.n_y,self.n_h)\n",
    "        assert self.b1.shape == (self.n_h,1)\n",
    "        assert self.b2.shape == (self.n_y,1)\n",
    "        self.params = {\"w1\":self.w1,\"b1\":self.b1,\"w2\":self.w2,\"b2\":self.b2}\n",
    "    \n",
    "    def lineer_func(X,W,b):\n",
    "        if X.shape[0] == (4,1): # Check x shape is (4,1) or not \n",
    "            pass\n",
    "        else:\n",
    "            X = X.reshape(-1,1)\n",
    "        assert X.shape == (n_x,1)\n",
    "        return np.dot(W,X)+b\n",
    "    \n",
    "    # Activation Functions\n",
    "    # Non-lineer part of a neuron \n",
    "    def non_lineer_func(self,Z,f='sigmoid'):\n",
    "        '''\n",
    "        Z : lineer calculation result \n",
    "        f : activation function \n",
    "        '''\n",
    "        if f == 'sigmoid':\n",
    "            score = 1 / (1 + np.exp(-Z))\n",
    "            return score\n",
    "        elif f == 'softmax': \n",
    "            score = np.exp(Z) / np.sum(np.exp(Z))  \n",
    "            return score \n",
    "        elif f== 'tanh':\n",
    "            score = np.tanh(Z)\n",
    "            return score \n",
    "        else:\n",
    "            raise Exception('Wrong Activation Function in Non-lineer!')\n",
    "    \n",
    "    def forward_propagation(self,X,y):\n",
    "        W1 = self.params[\"w1\"]\n",
    "        b1 = self.params[\"b1\"]\n",
    "        W2 = self.params[\"w2\"]\n",
    "        b2 = self.params[\"b2\"]\n",
    "        \n",
    "        assert len(y) == 1 \n",
    "        assert W1.shape[1] ==  X.shape[0]\n",
    "        \n",
    "        Z1 = lineer_func(X,W1,b1)\n",
    "        A1 = non_lineer_func(Z1,f='tanh')\n",
    "\n",
    "        assert W2.T.shape[1] == A1.shape[0]\n",
    "\n",
    "        Z2 = lineer_func(A1,W2,b2)\n",
    "        yhat = non_lineer_func(Z2,f='softmax')\n",
    "\n",
    "        loss = np.multiply(np.log(yhat),y) + np.multiply(np.log(1-yhat),1-y)\n",
    "\n",
    "        self.cache = {\"Z1\":Z1,\"A1\":A1,\"Z2\":Z2,\"yhat\":yhat}\n",
    "\n",
    "        return yhat,loss\n",
    "    \n",
    "    def tanh_derivative(A):\n",
    "        '''\n",
    "        Args: You need to give Ai to find derivate of Zi\n",
    "        '''\n",
    "        return (1- np.power(A,2))\n",
    "    \n",
    "    def sigmoid_derivative(A):\n",
    "        '''\n",
    "        Args: You need to give Ai to find derivate of Zi\n",
    "        '''\n",
    "        return np.multiply(A,(1-A))\n",
    "    \n",
    "    def back_propagation(self,X,yhat):\n",
    "        '''\n",
    "        compute derivative and update weights&biases\n",
    "        '''\n",
    "        assert len(y) == 1 \n",
    "        assert yhat.shape == (self.n_y,1)\n",
    "        m = self.sample_size\n",
    "        W1 = self.params[\"w1\"]\n",
    "        W2 = self.params[\"w2\"]\n",
    "        \n",
    "        A1 = self.cache[\"A1\"]\n",
    "        yhat = self.cache[\"yhat\"]\n",
    "        # backward\n",
    "        dz2 =  - self.y \n",
    "        print(\"dZ2:\\n\",dz2,\"\\t Shape:\",dz2.shape)\n",
    "        dw2 = np.dot(dz2,A1.T) / m\n",
    "        db2 = np.sum(dz2,axis=1,keepdims=True) / m\n",
    "\n",
    "        dz1 = np.multiply(np.dot(W2.T,dz2), tanh_derivative(A1))\n",
    "\n",
    "        dw1 = np.dot(dz1,self.X.T) / m \n",
    "        db1 = np.sum(dz1,axis=1,keepdims=True) / m\n",
    "        \n",
    "        self.grads = {\"dw1\":dw1,\"db1\":db1,\"dw2\":dw2,\"db2\":db2}\n",
    "\n",
    "    def update_params(self):\n",
    "        lr = self.learning_rate\n",
    "        # Weight and biases \n",
    "        W1 = self.params[\"w1\"]\n",
    "        b1 = self.params[\"b1\"]\n",
    "        W2 = self.params[\"w2\"]\n",
    "        b2 = self.params[\"b2\"]\n",
    "\n",
    "        # Gradients\n",
    "        dw1 = self.grads[\"dw1\"]\n",
    "        db1 = self.grads[\"db1\"]\n",
    "        dw2 = self.grads[\"dw2\"]\n",
    "        db2 = self.grads[\"db2\"]\n",
    "\n",
    "        # Update parameters according to given parameters and learning rate \n",
    "        w1 = W1 - lr * dw1\n",
    "        b1 = b1 - lr * db1\n",
    "        w2 = W2 - lr * dw2\n",
    "        b2 = b2 - lr * db2\n",
    "\n",
    "        self.params = {\"w1\":w1,\"b1\":b1,\"w2\":w2,\"b2\":b2}\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        print(\"INFO: Training is began...\")\n",
    "        #trainable_count = len(self.params[\"w1\"].reshape(-1)) + len(self.params[\"b1\"].reshape(-1)) + len(self.params[\"w2\"].reshape(-1)) + len(self.params[\"b2\"].reshape(-1))\n",
    "        #print(f\"INFO: Layer Count:{len(self.layers)-1}\\n Learning Rate:{self.learning_rate}\\n Epochs:{self.iterations} \\nTrainable Parameters:{trainable_count}\")\n",
    "        self.init_params()\n",
    "        self.sample_size = X.shape[1]\n",
    "        for epoch in range(self.iterations):\n",
    "            for i in range(self.sample_size):\n",
    "                self.X = X[:,i].reshape(-1,1)\n",
    "                self.y = y[i].reshape(-1,1)\n",
    "                yhat,loss = self.forward_propagation()\n",
    "                self.back_propagation(yhat)\n",
    "                self.update_params()\n",
    "\n",
    "                self.losses.append(loss)\n",
    "            cost = -1 * np.mean(self.losses)\n",
    "            cost = float(np.squeeze(cost)) # be sure to 1 dimension\n",
    "            assert isinstance(cost,float)\n",
    "            \n",
    "            losses = [] # clear losses list \n",
    "            if epoch % 10 == 0:\n",
    "                print(\"Epoch {} \\t ---> \\t Cost: \".format(epoch+1,cost)) # Show cost for every 10 epoch\n",
    "\n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        Makes prediction using model on test data\n",
    "        '''\n",
    "        yhat,_ = self.forward_propagation()\n",
    "        return yhat\n",
    "    \n",
    "    def acc(self,y,yhat):\n",
    "        acc = int( sum(y==yhat) / len(y) * 100)\n",
    "        return acc \n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.loss)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylable(\"logloss\")\n",
    "        plt.title(\"Loss cure for training\")\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain.shape: (4, 120) \tytrain.shape: (1, 120)\n"
     ]
    }
   ],
   "source": [
    "print(\"Xtrain.shape:\",Xtrain.shape,\"\\tytrain.shape:\",ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': array([[0.83559413, 0.27079733, 0.83382243, 0.91787646],\n",
       "        [0.79953727, 0.9113884 , 0.01688752, 0.1386197 ],\n",
       "        [0.18531292, 0.59344484, 0.03649412, 0.92465073],\n",
       "        [0.08828581, 0.19371289, 0.21318629, 0.67314935],\n",
       "        [0.21113236, 0.8722589 , 0.44307872, 0.66029233]]),\n",
       " 'b1': array([[0.16676669],\n",
       "        [0.51089048],\n",
       "        [0.01564739],\n",
       "        [0.1772931 ],\n",
       "        [0.70499102]]),\n",
       " 'w2': array([[0.64639801, 0.42738247, 0.44336621, 0.29746506, 0.41309084],\n",
       "        [0.11633294, 0.12445723, 0.51976989, 0.83939216, 0.72035919],\n",
       "        [0.76636861, 0.13839453, 0.0664479 , 0.70260681, 0.85438491]]),\n",
       " 'b2': array([[0.93587434],\n",
       "        [0.50973299],\n",
       "        [0.31619615]])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_.shape:(4, 1)\ty_.shape:(1, 1)\n"
     ]
    }
   ],
   "source": [
    "X_,y_ = Xtrain[:,0].reshape(-1,1),ytrain[:,0].reshape(-1,1)\n",
    "print(f\"X_.shape:{X_.shape}\\ty_.shape:{y_.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lineer_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-95c199c301f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-6746abfc31fb>\u001b[0m in \u001b[0;36mforward_propagation\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m  \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[0mZ1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlineer_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[0mA1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnon_lineer_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tanh'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lineer_func' is not defined"
     ]
    }
   ],
   "source": [
    "nn.forward_propagation(X_,y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.params[\"w1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.params[\"w1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.cache[\"Z2\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STRUCTURE OF ASSIGNMENT \n",
    "'''\n",
    "losses = [] # intialize loss array \n",
    "init_params() # initialize weights & biases \n",
    "for epoch in epochs: \n",
    "    yhat,_ = forward_propagation(X) # predict something in case of current weights & biases \n",
    "    loss = cross_entropy(y,yhat) # calculate loss \n",
    "    backward_propagation() # Calculate gradients \n",
    "    update_params() # Update parameters\n",
    "    losses.append(loss) # add loss \n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        cost = calc_cost(losses) # calculate cost \n",
    "        print(\"Epoch {} \\t ---> \\t Cost: \".format(epoch,cost)) # Show cost for every 10 epoch\n",
    "''' \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
